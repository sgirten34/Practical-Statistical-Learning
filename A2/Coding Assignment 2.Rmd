---
title: "Coding Assignment 2"
author: "Scott Girten"
date: "9/18/2023"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Part 1: Implement Lasso


### One-Variable Lasso
```{r one-variable lasso}

one_var_lasso = function(r, z, lam) {
  
  a = t(r)%*%z / (t(z)%*%z)
  eta = 2*length(r)*lam / (t(z)%*%z)
  

  if(a > eta/2){
    x_star = a-(eta/2)
    }
  else if(abs(a) <= eta/2){
    x_star = 0
    }
  else if(a < -1*eta/2){
    x_star = a+(eta/2)
    }

  return(x_star)

}

```

### Coordinate Descent

```{r coordinate descent}

MyLasso = function(X, y, lam.seq, maxit = 100) {
    
    # Input
    # X: n-by-p design matrix without the intercept 
    # y: n-by-1 response vector 
    # lam.seq: sequence of lambda values (arranged from large to small)
    # maxit: number of updates for each lambda 
    
    # Output
    # B: a (p+1)-by-length(lam.seq) coefficient matrix 
    #    with the first row being the intercept sequence

    n = length(y)
    p = dim(X)[2]
    nlam = length(lam.seq)
    B = matrix(0, ncol = nlam, nrow = (p+1))
    rownames(B) = c("Intercept", colnames(X)) 

    ##############################
    # YOUR CODE: 
    # (1) new.X = centered & scaled X;

    new.X = scale(X)
    
    ##############################

    # Initialize coef vector b and residual vector r
    b = rep(0, p)
    r = y
    
    # Triple nested loop
    for (m in 1:nlam) {
      for (step in 1:maxit) {
        for (j in 1:p) {
          r = r + (new.X[, j] * b[j])
          b[j] = one_var_lasso(r, new.X[, j], lam.seq[m])
          r = r - new.X[, j] * b[j]
        }
      }
      B[-1, m] = b
      
    }
    
    ##############################
    # YOUR CODE:
    # scale back the coefficients;
    # update the intercepts stored in B[1, ]
    
    # Scale back the coefficients
    B[2:14, ] = B[2:14, ] / attr(new.X, 'scaled:scale') * attr(new.X, 'scaled:center')
    
    # Calculate A (Intercept)
    A = mean(y) - colSums(B[2:14, ] * attr(new.X, 'scaled:center') / attr(new.X, 'scaled:scale'))
    
    # Add intercept to matrix
    B[1, ] = A
    
    ##############################
    
    return(B)
}



```


## Test My Function

```{r test function}
myData = read.csv("Coding2_Data.csv")
X = as.matrix(myData[, -14])
y = myData$Y
lam.seq = exp(seq(-1, -8, length.out = 80))
myout = MyLasso(X, y, lam.seq)


```


```{r test vs glmnet}
library(glmnet)
lasso.fit = glmnet(X, y, alpha = 1, lambda = lam.seq)
max(abs(coef(lasso.fit) - myout))

check = as.matrix(coef(lasso.fit))
```


```{r path plot}

x.index = log(lam.seq)
beta = myout[-1, ]  # beta is a 13-by-80 matrix
matplot(x.index, t(beta),
        xlim = c(min(x.index), max(x.index)),
        lty = 1,
        xlab = "Log Lambda",
        ylab = "Coefficients",
        type="l", 
        lwd = 1)
# You can add variable names to each path
var.names = colnames(X)
nvar = length(var.names)
xpos = rep(min(x.index), nvar)
ypos = beta[, ncol(beta)]
text(xpos, ypos, var.names, cex=0.5, pos=2)

```



```{r glmnet plot}

plot(lasso.fit, xvar = "lambda")

```

***

# Part II: Simulation Study

## Case I

```{r message=FALSE, warning=FALSE}
#clear memory
rm(list = ls())

library(glmnet) 
library(pls)
library(tidyverse)

myData = read.csv("Coding2_Data2.csv", header = TRUE)

X = data.matrix(myData[,-1])  
Y = myData[,1] 


```


```{r simulation}
# Store variables
iter = 50
n = length(Y)

# Data frame to hold MSPE for each iteration
results = tibble(Iteration = c(1:iter),
                 `Full Regression` = rep(0, iter),
                 Ridge.min = rep(0, iter),
                 Lasso.min = rep(0, iter),
                 Lasso.1se = rep(0, iter),
                 L.Refit = rep(0, iter),
                 PCR = rep(0, iter))


# Begin simulation
for(i in 1:iter){
  
  # IDs for train/test split
  test.id = sample(c(1:nrow(X)), size = c(nrow(X)*0.25))

  
  # Full regression model
  full.model = lm(Y ~ ., data = myData[-test.id, ])
  Ytest.pred = predict(full.model, newdata = myData[test.id, ])
  results[i, "Full Regression"] = mean((myData$Y[test.id] - Ytest.pred)^2)
  
  
  # Ridge Regression
  mylasso.lambda.seq = exp(seq(-10, 1, length.out = 100))
  cv.out = cv.glmnet(X[-test.id, ], Y[-test.id], alpha = 0, 
                     lambda = mylasso.lambda.seq)
  
  best.lam = cv.out$lambda.min
  Ytest.pred = predict(cv.out, s = best.lam, newx = X[test.id, ])
  results[i, "Ridge.min"] = mean((Y[test.id] - Ytest.pred)^2)
  
  
  # Lasso Regression
  # Lambda.min
  cv.out = cv.glmnet(X[-test.id, ], Y[-test.id], alpha = 1)
  best.lam = cv.out$lambda.min
  Ytest.pred = predict(cv.out, s = best.lam, newx = X[test.id, ])
  results[i, 'Lasso.min'] = mean((Y[test.id] - Ytest.pred)^2)
  
  # Lambda.1se
  best.lam = cv.out$lambda.1se
  Ytest.pred = predict(cv.out, s = best.lam, newx = X[test.id, ])
  results[i, "Lasso.1se"] = mean((Y[test.id] - Ytest.pred)^2)
  
  # Lasso refit
  mylasso.coef = predict(cv.out, s = best.lam, type = "coefficients")
  var.sel = row.names(mylasso.coef)[which(mylasso.coef != 0)[-1]]
  mylasso.refit = lm(Y ~ ., myData[-test.id, c("Y", var.sel)])
  Ytest.pred = predict(mylasso.refit, newdata = myData[test.id, ])
  results[i, "L.Refit"] = mean((Ytest.pred - Y[test.id])^2)
  
  
  # PCR
  mypcr = pcr(Y ~ ., data= myData[-test.id, ], validation="CV", scale=TRUE)
  CVerr = RMSEP(mypcr)$val[1, , ]
  adjCVerr = RMSEP(mypcr)$val[2, , ]
  best.ncomp = which.min(CVerr) - 1 
  
  if (best.ncomp==0) {
      Ytest.pred = mean(myData$Y[-test.id])
    } else {
      Ytest.pred = predict(mypcr, myData[test.id,], ncomp=best.ncomp)
    }
  results[i, "PCR"] = mean((Ytest.pred - myData$Y[test.id])^2)
  

}



```


```{r results visual}
# strip chart for results 
df_p = results %>% 
  pivot_longer(cols = 2:7, names_to = "Model", values_to = "Error")

p = ggplot(df_p, aes(x = Model, y = Error, color = Model)) +
  geom_boxplot() +
  geom_jitter(width = 0.25, alpha = 0.4) +
  #geom_point() +
  theme_minimal() + 
  theme(legend.position = 'none') +
  #facet_wrap(. ~ Model, nrow = 1) +
  labs(y = 'MSPE',
       X = NULL,
       title = 'Mean Squared Prediction Error Comparison of 6 Regression Models')

p
```

*Which procedure or procedures yield the best performance in terms of MSPE?*  
The Ridge.min and the Lasso.min models have the best performance in terms of MSPE.  

*Conversely, which procedure or procedures show the poorest performance?*  
The Lasso.1se model has the poorest performance for MSPE.  

*In the context of Lasso regression, which procedure, Lasso.min or Lasso.1se, yields a better MSPE?*  
Lasso.min regression has a better MSPE performance.  

*Is refitting advantageous in this case? In other words, does L.Refit outperform Lasso.1se?*
Refitting does not seem to be advantageous in this case.  There is a very slight improvement in performance for the refitted model with a slightly lower mean performance, but generally the refitted model's performance is in line with the Lasso.1se performance.  

*Is variable selection or shrinkage warranted for this particular dataset? To clarify, do you find the performance of the Full model to be comparable to, or divergent from, the best-performing procedure among the other five?*  
I think that variable selection or shrinkage is warranted for this particular dataset.  The Lasso.min and Ridge.min have similar performance for the choice of best model.  Additionally, both the Lasso.min and Ridge.min models have better performance (lower mean and and IQR shifted lower) than the full model and would likely perform better than the full regression model.  



## Case II

```{r case 2 data}
#clear memory
rm(list = ls())

#library(glmnet) 
#library(pls)
#library(tidyverse)

myData = read.csv("Coding2_Data3.csv", header = TRUE)

X = data.matrix(myData[,-1])  
Y = myData[,1] 

```


```{r}
# Store variables
iter = 50
n = length(Y)

# Data frame to hold MSPE for each iteration
results = tibble(Iteration = c(1:iter),
                 Ridge.min = rep(0, iter),
                 Lasso.min = rep(0, iter),
                 Lasso.1se = rep(0, iter),
                 L.Refit = rep(0, iter),
                 PCR = rep(0, iter))


# Begin simulation
for(i in 1:iter){
  
  # IDs for train/test split
  test.id = sample(c(1:nrow(X)), size = c(nrow(X)*0.25))

  
  # Full regression model
  # full.model = lm(Y ~ ., data = myData[-test.id, ])
  # Ytest.pred = predict(full.model, newdata = myData[test.id, ])
  # results[i, "Full Regression"] = mean((myData$Y[test.id] - Ytest.pred)^2)
  # 
  
  # Ridge Regression
  mylasso.lambda.seq = exp(seq(-10, 1, length.out = 100))
  cv.out = cv.glmnet(X[-test.id, ], Y[-test.id], alpha = 0, 
                     lambda = mylasso.lambda.seq)
  
  best.lam = cv.out$lambda.min
  Ytest.pred = predict(cv.out, s = best.lam, newx = X[test.id, ])
  results[i, "Ridge.min"] = mean((Y[test.id] - Ytest.pred)^2)
  
  
  # Lasso Regression
  # Lambda.min
  cv.out = cv.glmnet(X[-test.id, ], Y[-test.id], alpha = 1)
  best.lam = cv.out$lambda.min
  Ytest.pred = predict(cv.out, s = best.lam, newx = X[test.id, ])
  results[i, 'Lasso.min'] = mean((Y[test.id] - Ytest.pred)^2)
  
  # Lambda.1se
  best.lam = cv.out$lambda.1se
  Ytest.pred = predict(cv.out, s = best.lam, newx = X[test.id, ])
  results[i, "Lasso.1se"] = mean((Y[test.id] - Ytest.pred)^2)
  
  # Lasso refit
  mylasso.coef = predict(cv.out, s = best.lam, type = "coefficients")
  var.sel = row.names(mylasso.coef)[which(mylasso.coef != 0)[-1]]
  mylasso.refit = lm(Y ~ ., myData[-test.id, c("Y", var.sel)])
  Ytest.pred = predict(mylasso.refit, newdata = myData[test.id, ])
  results[i, "L.Refit"] = mean((Ytest.pred - Y[test.id])^2)
  
  
  # PCR
  mypcr = pcr(Y ~ ., data= myData[-test.id, ], validation="CV", scale=TRUE)
  CVerr = RMSEP(mypcr)$val[1, , ]
  adjCVerr = RMSEP(mypcr)$val[2, , ]
  best.ncomp = which.min(CVerr) - 1 
  
  if (best.ncomp==0) {
      Ytest.pred = mean(myData$Y[-test.id])
    } else {
      Ytest.pred = predict(mypcr, myData[test.id,], ncomp=best.ncomp)
    }
  results[i, "PCR"] = mean((Ytest.pred - myData$Y[test.id])^2)
  

}


```

```{r case II visual}
df_p2 = results %>% 
  pivot_longer(cols = 2:6, names_to = "Model", values_to = "Error")

p2 = ggplot(df_p2, aes(x = Model, y = Error, color = Model)) +
  geom_boxplot() +
  geom_jitter(width = 0.25, alpha = 0.4) +
  #geom_point() +
  theme_minimal() + 
  theme(legend.position = 'none') +
  #facet_wrap(. ~ Model, nrow = 1) +
  labs(y = 'MSPE',
       X = NULL,
       title = 'Mean Squared Prediction Error Comparison of 5 Regression Models')

p2
```

*Which procedure or procedures yield the best performance in terms of MSPE?*  
The Lasso.min model produced the best performance in terms of MSPE.  

*Conversely, which procedure or procedures show the poorest performance?*  
The PCR model showed the poorest performance for MSPE.  

*Have you observed any procedure or procedures that performed well in Case I but exhibited poorer performance in Case II, or vice versa? If so, please offer an explanation.*  


*Given that Coding2_Data3.csv includes all features found in Coding2_Data2.csv, one might anticipate that the best MSPE in Case II would be equal to or lower than the best MSPE in Case I. Do your simulation results corroborate this expectation? If not, please offer an explanation.*





